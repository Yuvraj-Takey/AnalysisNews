
[ 2024-09-10 14:58:01,427 ] - DEBUG - root - main.py:17 - Initiated Preprocessing Module

[ 2024-09-10 14:58:01,427 ] - DEBUG - root - processDataIngestion.py:11 - Document Ingestion class initialized

[ 2024-09-10 14:58:06,473 ] - DEBUG - root - processDataIngestion.py:33 - Successfully loaded the input file to the memory

[ 2024-09-10 14:58:06,473 ] - DEBUG - root - processEdit.py:10 - Process Edit Message class initialized

[ 2024-09-10 14:58:06,473 ] - DEBUG - root - processJSON.py:25 - JSON Pre-Processing class initiated

[ 2024-09-10 14:58:06,473 ] - DEBUG - root - utils.py:31 - [INFO] : Loading the configuration data

[ 2024-09-10 14:58:06,481 ] - INFO - root - utils.py:37 - Config Yaml loaded succesfully.

[ 2024-09-10 14:58:06,496 ] - DEBUG - root - processEdit.py:85 - Operation on-line extraction has initiated

[ 2024-09-10 14:58:06,496 ] - DEBUG - root - processEdit.py:32 - Started operation on line level adjustment

[ 2024-09-10 14:58:06,496 ] - DEBUG - root - processEdit.py:67 - Forwarding JSON object for futher pre-processing task

[ 2024-09-10 14:58:06,497 ] - DEBUG - root - processJSON.py:386 - Dealing with missing columns and sheets

[ 2024-09-10 14:58:06,505 ] - DEBUG - root - processJSON.py:55 - Started extraction of claim IDs from the input file

[ 2024-09-10 14:58:06,507 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,508 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,513 ] - DEBUG - root - processJSON.py:308 - Working on Hippa Attribute

[ 2024-09-10 14:58:06,516 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,517 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,530 ] - DEBUG - root - processJSON.py:254 - max date replacement task is started

[ 2024-09-10 14:58:06,532 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,533 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,537 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,539 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,578 ] - DEBUG - root - processJSON.py:336 - Dealing with Raw data from COB

[ 2024-09-10 14:58:06,580 ] - DEBUG - root - processJSON.py:254 - max date replacement task is started

[ 2024-09-10 14:58:06,582 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,582 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,603 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,603 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,609 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,610 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,622 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,623 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,642 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,643 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,665 ] - DEBUG - root - processJSON.py:87 - Data persistance is started

[ 2024-09-10 14:58:06,671 ] - INFO - root - processJSON.py:101 - Data JSON is stored at the location : C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\artifacts\output\24B121111110_wm_invalidproviderselection.json

[ 2024-09-10 14:58:06,672 ] - DEBUG - root - processEdit.py:32 - Started operation on line level adjustment

[ 2024-09-10 14:58:06,673 ] - DEBUG - root - processEdit.py:67 - Forwarding JSON object for futher pre-processing task

[ 2024-09-10 14:58:06,673 ] - DEBUG - root - processJSON.py:386 - Dealing with missing columns and sheets

[ 2024-09-10 14:58:06,680 ] - DEBUG - root - processJSON.py:55 - Started extraction of claim IDs from the input file

[ 2024-09-10 14:58:06,681 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,682 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,686 ] - DEBUG - root - processJSON.py:308 - Working on Hippa Attribute

[ 2024-09-10 14:58:06,690 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,691 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,704 ] - DEBUG - root - processJSON.py:254 - max date replacement task is started

[ 2024-09-10 14:58:06,706 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,708 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,713 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,714 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,763 ] - DEBUG - root - processJSON.py:336 - Dealing with Raw data from COB

[ 2024-09-10 14:58:06,764 ] - DEBUG - root - processJSON.py:254 - max date replacement task is started

[ 2024-09-10 14:58:06,766 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,767 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,796 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,796 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,804 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,805 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,820 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,821 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,843 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:58:06,843 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:58:06,878 ] - DEBUG - root - processJSON.py:87 - Data persistance is started

[ 2024-09-10 14:58:06,889 ] - INFO - root - processJSON.py:101 - Data JSON is stored at the location : C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\artifacts\output\24B121111110_wm_invalidproviderselectiononline1.json

[ 2024-09-10 14:58:06,891 ] - DEBUG - root - dataFiltration.py:10 - filterData class initialized

[ 2024-09-10 14:58:06,910 ] - DEBUG - root - dataFiltration.py:222 - Got ['24B121111110_wm_invalidproviderselection.json', '24B121111110_wm_invalidproviderselectiononline1.json'] for filtering

[ 2024-09-10 14:58:06,913 ] - DEBUG - root - dataFiltration.py:234 - Started filtering 24B121111110_wm_invalidproviderselection

[ 2024-09-10 14:58:06,913 ] - DEBUG - root - dataFiltration.py:165 - Started filtering and appying 2 filter/s

[ 2024-09-10 14:58:06,930 ] - DEBUG - root - dataFiltration.py:139 - applyFilter initialized for input1.providerdetails.npi == 

[ 2024-09-10 14:58:06,992 ] - ERROR - root - main.py:42 - There seems to be a issue in Preprocessing - Traceback (most recent call last):
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 144, in applyFilter
    dataFrames[filterSheet] = df[df.apply(lambda row: eval(f"{row[filterColumn]}"+operator+f"{actualValue}"), axis=1)]
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\base39\lib\site-packages\pandas\core\frame.py", line 10361, in apply
    return op.apply().__finalize__(self, method="apply")
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\base39\lib\site-packages\pandas\core\apply.py", line 916, in apply
    return self.apply_standard()
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\base39\lib\site-packages\pandas\core\apply.py", line 1063, in apply_standard
    results, res_index = self.apply_series_generator()
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\base39\lib\site-packages\pandas\core\apply.py", line 1081, in apply_series_generator
    results[i] = self.func(v, *self.args, **self.kwargs)
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 144, in <lambda>
    dataFrames[filterSheet] = df[df.apply(lambda row: eval(f"{row[filterColumn]}"+operator+f"{actualValue}"), axis=1)]
  File "<string>", line 1
    1548200028.0==
                 ^
SyntaxError: unexpected EOF while parsing

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 172, in filterInputFile
    dataFrames = self.applyFilter(dataFrames, filterSheet, filterColumn, operator, actualValue) # applying a filter
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 148, in applyFilter
    raise Exception()
Exception

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 242, in getFilteredInputFile
    dataFrames = self.filterInputFile(inputFile, filters)
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 174, in filterInputFile
    raise Exception()
Exception

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\main.py", line 31, in inputPreprocessUnit
    filterObj.getFilteredInputFile(jsonFileName)
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 252, in getFilteredInputFile
    raise Exception()
Exception

