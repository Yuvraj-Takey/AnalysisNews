
[ 2024-09-10 14:56:36,414 ] - DEBUG - root - main.py:17 - Initiated Preprocessing Module

[ 2024-09-10 14:56:36,414 ] - DEBUG - root - processDataIngestion.py:11 - Document Ingestion class initialized

[ 2024-09-10 14:56:43,315 ] - DEBUG - root - processDataIngestion.py:33 - Successfully loaded the input file to the memory

[ 2024-09-10 14:56:43,315 ] - DEBUG - root - processEdit.py:10 - Process Edit Message class initialized

[ 2024-09-10 14:56:43,315 ] - DEBUG - root - processJSON.py:25 - JSON Pre-Processing class initiated

[ 2024-09-10 14:56:43,315 ] - DEBUG - root - utils.py:31 - [INFO] : Loading the configuration data

[ 2024-09-10 14:56:43,327 ] - INFO - root - utils.py:37 - Config Yaml loaded succesfully.

[ 2024-09-10 14:56:43,360 ] - DEBUG - root - processEdit.py:85 - Operation on-line extraction has initiated

[ 2024-09-10 14:56:43,361 ] - DEBUG - root - processEdit.py:32 - Started operation on line level adjustment

[ 2024-09-10 14:56:43,361 ] - DEBUG - root - processEdit.py:67 - Forwarding JSON object for futher pre-processing task

[ 2024-09-10 14:56:43,362 ] - DEBUG - root - processJSON.py:386 - Dealing with missing columns and sheets

[ 2024-09-10 14:56:43,384 ] - DEBUG - root - processJSON.py:55 - Started extraction of claim IDs from the input file

[ 2024-09-10 14:56:43,387 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,390 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:43,409 ] - DEBUG - root - processJSON.py:308 - Working on Hippa Attribute

[ 2024-09-10 14:56:43,418 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,421 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:43,460 ] - DEBUG - root - processJSON.py:254 - max date replacement task is started

[ 2024-09-10 14:56:43,463 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,465 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:43,482 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,484 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:43,613 ] - DEBUG - root - processJSON.py:336 - Dealing with Raw data from COB

[ 2024-09-10 14:56:43,615 ] - DEBUG - root - processJSON.py:254 - max date replacement task is started

[ 2024-09-10 14:56:43,621 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,622 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:43,696 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,698 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:43,717 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,718 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:43,764 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,765 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:43,828 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,830 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:43,906 ] - DEBUG - root - processJSON.py:87 - Data persistance is started

[ 2024-09-10 14:56:43,918 ] - INFO - root - processJSON.py:101 - Data JSON is stored at the location : C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\artifacts\output\24B121111110_wm_invalidproviderselection.json

[ 2024-09-10 14:56:43,920 ] - DEBUG - root - processEdit.py:32 - Started operation on line level adjustment

[ 2024-09-10 14:56:43,921 ] - DEBUG - root - processEdit.py:67 - Forwarding JSON object for futher pre-processing task

[ 2024-09-10 14:56:43,922 ] - DEBUG - root - processJSON.py:386 - Dealing with missing columns and sheets

[ 2024-09-10 14:56:43,947 ] - DEBUG - root - processJSON.py:55 - Started extraction of claim IDs from the input file

[ 2024-09-10 14:56:43,949 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,954 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:43,968 ] - DEBUG - root - processJSON.py:308 - Working on Hippa Attribute

[ 2024-09-10 14:56:43,984 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:43,987 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:44,044 ] - DEBUG - root - processJSON.py:254 - max date replacement task is started

[ 2024-09-10 14:56:44,048 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:44,055 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:44,076 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:44,078 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:44,214 ] - DEBUG - root - processJSON.py:336 - Dealing with Raw data from COB

[ 2024-09-10 14:56:44,215 ] - DEBUG - root - processJSON.py:254 - max date replacement task is started

[ 2024-09-10 14:56:44,222 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:44,224 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:44,294 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:44,295 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:44,311 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:44,313 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:44,355 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:44,357 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:44,413 ] - DEBUG - root - processJSON.py:205 - Data cleaning task is started

[ 2024-09-10 14:56:44,414 ] - DEBUG - root - processJSON.py:117 - Data type conversion task is started

[ 2024-09-10 14:56:44,495 ] - DEBUG - root - processJSON.py:87 - Data persistance is started

[ 2024-09-10 14:56:44,510 ] - INFO - root - processJSON.py:101 - Data JSON is stored at the location : C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\artifacts\output\24B121111110_wm_invalidproviderselectiononline1.json

[ 2024-09-10 14:56:44,511 ] - DEBUG - root - dataFiltration.py:10 - filterData class initialized

[ 2024-09-10 14:56:44,528 ] - DEBUG - root - dataFiltration.py:222 - Got ['24B121111110_wm_invalidproviderselection.json', '24B121111110_wm_invalidproviderselectiononline1.json'] for filtering

[ 2024-09-10 14:56:44,530 ] - DEBUG - root - dataFiltration.py:234 - Started filtering 24B121111110_wm_invalidproviderselection

[ 2024-09-10 14:56:44,530 ] - DEBUG - root - dataFiltration.py:165 - Started filtering and appying 2 filter/s

[ 2024-09-10 14:56:44,543 ] - ERROR - root - main.py:42 - There seems to be a issue in Preprocessing - Traceback (most recent call last):
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 111, in convertToDataframe
    dataFrames['24B121111110'][sheet] = pd.DataFrame(inputFile['24B121111110'][sheet]) # sheet name as key and its data(in form of dataframe) as value
KeyError: '24B121111110'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 242, in getFilteredInputFile
    dataFrames = self.filterInputFile(inputFile, filters)
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 167, in filterInputFile
    dataFrames = self.convertToDataframe(inputFile) # converts dictionary of dataframes
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 118, in convertToDataframe
    raise Exception()
Exception

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\main.py", line 31, in inputPreprocessUnit
    filterObj.getFilteredInputFile(jsonFileName)
  File "C:\Users\2332220\OneDrive - Cognizant\Desktop\GenAI_24\0_Experimentals\0_Expr_Test\1_EXCEL_JSON\1_Input_PreProcessing\components\dataFiltration.py", line 252, in getFilteredInputFile
    raise Exception()
Exception

